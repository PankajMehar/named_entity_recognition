{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Named Entities Extraction From Online News</h1><br><b><i>Python Notebook 1 of 3</i></b><br>The Following notebook contains all necessary steps for data pre-processing in order to produce the Gold Copus.\n",
    "<br>The steps are in the followig order:<br>1. Load raw data<br>2. Split content into individual documents<br>3. Extract news content<br>4. Feed each document into SpaCy (open source laguange processing library)<br>5. Save each tagged document into a seperate file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load Raw Data, Split into document and Extract content</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Preprocess files.\n",
    "import os, re, glob\n",
    "path = '.\\source\\*.txt'   \n",
    "files=glob.glob(path)   \n",
    "document = {}\n",
    "n=0\n",
    "#Go through every .txt file in the source file directory\n",
    "for file in files: \n",
    "    f = open(file,'r')\n",
    "    raw = f.read()\n",
    "    #Documents are split by multiple underscores in one file - we use the next line to split into individual documents\n",
    "    docs = raw.split(\"__________________________________________________________\")\n",
    "    for i in range(len(docs)):\n",
    "        text = ''\n",
    "        line_start = False\n",
    "        #Split each line and do any required cleaning here\n",
    "        for line in docs[i].split('\\n'):\n",
    "            if(re.match(r'Full text: ',line)):\n",
    "                line_start = True\n",
    "                line = re.sub(r'Full text: ','',line)\n",
    "                line = re.sub(r'Credit: Staff ','',line)\n",
    "            if line_start:\n",
    "                text += line + \"\\n\"\n",
    "        document[n] = text\n",
    "        n+=1\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check corpus size - number of documents produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Size: 2116 articles\n",
      "\n",
      "Sample Document:\n",
      "Workers at auto-body shops deliberately damaged cars; installed used parts but billed for new ones; or invoiced for phantom repairs, according to an investigation by a Canadian insurer that is calling on government to help in curbing the problem.\n",
      "Aviva Canada found about half the total expenses submitted for repairs to crashed vehicles during its investigation in Ontario were bogus - an amount the company estimates adds up to hundreds of millions of dollars a year.\n",
      "\"Nobody has ever really sample\n"
     ]
    }
   ],
   "source": [
    "#Print Corpus Size\n",
    "print(\"Corpus Size:\",len(document),\"articles\\n\")\n",
    "\n",
    "#Preview sample document\n",
    "print(\"Sample Document:\")\n",
    "print(document[114][0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prepare Tagged Dataset Using Spacy Open Source Application for Natural Language Processing.</h3><br>You will need to open your Anaconda promt and execute the following 2 commands:<br>\n",
    "- pip install spacy<br>\n",
    "- python -m spacy download xx_ent_wiki_sm\n",
    "- Restart jupyter notebook using command: jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will generate our tagged corpus and save it in the form of <b><i>word pos iob-ner</i></b> into external files. This will facilitate the process of manually tagging and validating the output of out NER corpus. For example Super Bowl and Sleeping Beauty were not correctly labeled as<br>Since this will be used as the Gold Standard Corpus it is important to ensure that the data is free of errors.<br>Also, the Spacy NER-Tags labels were not exactly the same, for example Spacy label for organization is ORG while in NLTK it is ORGANIZATION. To adhere to the same NLTK format, manual formating the format to the NLTK scheme was required in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "#In the following steps we used the pre-trained SpaCy model 'en_core_web_sm' to tag the dataset.\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "#the following mapping is to ensure our tag dataset contains the same labels used in most NLP entities systems.\n",
    "entLabelDict = {'ORG':'ORGANIZATION',\\\n",
    "                'PERSON':'PERSON',\\\n",
    "                'LOC':'LOCATION',\\\n",
    "                'DATE':'DATE',\\\n",
    "                'TIME':'TIME',\\\n",
    "                'MONEY':'MONEY',\\\n",
    "                'PERCENT':'PERCENT',\\\n",
    "                'FACILITY':'FACILITY',\\\n",
    "                'GPE':'GPE'}\n",
    "\n",
    "#Create NER_Tagger Function to resuse for pre-processing all documents into tagged corpus.\n",
    "def NER_Tagger(doc):\n",
    "    sents = nltk.sent_tokenize(doc)\n",
    "    sentences = ''\n",
    "    #Process each sentence \n",
    "    for sent in sents:\n",
    "        #sent = re.sub(r'','',sent)\n",
    "        tagged_sent = nlp(sent.strip())\n",
    "        tokens = ([t for t in tagged_sent])\n",
    "        #for each sentence return word,pos,iob-NER\n",
    "        if(len(tokens) > 3):\n",
    "            for t in tokens:\n",
    "                if(str(t) != '\\n' and str(t.tag_) != '_SP' and str(t) != '' and str(t) != ' ' and str(t.tag) != ' ' and str(t.tag) != ''):\n",
    "                    if(t.ent_type_ in entLabelDict.keys()): \n",
    "                        word = str(t) +' '+ str(t.tag_) + ' ' + str(t.ent_iob_) + '-' + str(entLabelDict[t.ent_type_]) \n",
    "                    else:\n",
    "                        word = str(t) +' '+ str(t.tag_) +' O' #+ str(t.ent_iob_)\n",
    "                    #append tagged words to sentence\n",
    "                    sentences += word + '\\n'\n",
    "            #add new line for sentence boundary (***required for Connl Corpus Reader)\n",
    "            sentences += '\\n'\n",
    "    #return all sentences associated with document\n",
    "    return sentences\n",
    "\n",
    "#loop through each document - tag using NER_Tagger and store into external corpus file\n",
    "\n",
    "#file counter\n",
    "n=0\n",
    "\n",
    "#open directory for files\n",
    "correction = open(\"dataset5/correction_needed.txt\",\"w\")\n",
    "\n",
    "#step into each document\n",
    "for doc in document:\n",
    "    n+=1\n",
    "    #process document using NER_Tagger Function above\n",
    "    text = str(NER_Tagger(document[n]))\n",
    "    #create file for saving current dataset file\n",
    "    f = open(\"dataset5/\"+str(n)+\".txt\",\"wb\")\n",
    "    #write into file using utf-8 encoding\n",
    "    f.write(NER_Tagger(document[n]).encode('utf-8'))\n",
    "    #close file\n",
    "    f.close()\n",
    "    #the following line was used to track additional entities that couldnt get labeled\n",
    "    #those entities and their referenced file were stored in a seprate file for easy retrieval called correction.txt\n",
    "    correction.write(\"\\n--- Document: \"+str(n)+\" ------------------------------------\\n\\n\")\n",
    "    #join all entity not correcty labeled into a variable\n",
    "    text_need_fixing = '\\n'.join(re.findall(r'.*NNP I\\n',text))\n",
    "    #append corrections into file\n",
    "    correction.write(str(text_need_fixing))\n",
    "#close correction file\n",
    "correction.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load pre-tagged corpus using ConllCorpusReader format</h3>Here we used the CoNNL Corpus reader function to assist in loading and ensuring data is in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import ConllCorpusReader\n",
    "my_corpus = ''\n",
    "my_corpus = ConllCorpusReader('.\\dataset4', '.*\\.txt', columntypes=('words', 'pos','chunk'), encoding='utf8')\n",
    "#my_corpus.iob_words('1.txt')\n",
    "\n",
    "#Extract Named Entities only - labels not containing the label O\n",
    "named_entities = [(e[0],e[2]) for e in my_corpus.iob_words() if e[2] not in ('O')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'B-ORGANIZATION'), ('Globe', 'I-ORGANIZATION'), ('and', 'I-ORGANIZATION'), ('Mail', 'I-ORGANIZATION'), ('National', 'B-ORGANIZATION'), ('Ballet', 'I-ORGANIZATION'), ('of', 'I-ORGANIZATION'), ('Canada', 'I-ORGANIZATION'), ('Canada', 'B-GPE'), ('Sophie', 'B-LOCATION'), ('Letendre', 'I-LOCATION'), ('the', 'B-ORGANIZATION'), ('Walter', 'I-ORGANIZATION'), ('Carsen', 'I-ORGANIZATION'), ('Centre', 'I-ORGANIZATION'), ('Toronto', 'B-GPE'), ('Feb.', 'B-DATE'), ('28', 'I-DATE'), ('to', 'I-DATE'), ('March', 'I-DATE'), ('4', 'I-DATE'), ('Letendre', 'B-ORGANIZATION'), ('The', 'B-ORGANIZATION'), ('Globe', 'I-ORGANIZATION'), ('and', 'I-ORGANIZATION'), ('Mail', 'I-ORGANIZATION'), ('late', 'B-DATE'), ('last', 'I-DATE'), ('month', 'I-DATE'), ('Today', 'B-DATE'), ('Letendre', 'B-ORGANIZATION'), ('Letendre', 'B-ORGANIZATION'), ('2006', 'B-DATE'), ('a', 'B-DATE'), ('year', 'I-DATE'), ('later', 'I-DATE'), ('Letendre', 'B-ORGANIZATION'), ('the', 'B-ORGANIZATION'), ('Canadian', 'I-ORGANIZATION'), ('Actors', 'I-ORGANIZATION'), (\"'\", 'I-ORGANIZATION'), ('Equity', 'I-ORGANIZATION'), ('Association', 'I-ORGANIZATION'), ('Letendre', 'B-ORGANIZATION'), ('Letendre', 'B-ORGANIZATION'), ('Letendre', 'B-ORGANIZATION'), ('Credit', 'B-ORGANIZATION'), (':', 'I-ORGANIZATION'), ('Staff', 'I-ORGANIZATION'), ('tomorrow', 'B-DATE')]\n"
     ]
    }
   ],
   "source": [
    "print(named_entities[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get some statitics on the named_entities</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities Count of Labels and Unique Words\n",
      "B-DATE,22093,882\n",
      "B-GPE,21036,2069\n",
      "B-LOCATION,1922,332\n",
      "B-MONEY,5893,1940\n",
      "B-ORGANIZATION,26639,5501\n",
      "B-PERCENT,40,34\n",
      "B-PERSON,30522,5694\n",
      "B-TIME,1887,280\n",
      "I-DATE,24643,827\n",
      "I-GPE,3824,423\n",
      "I-LOCATION,1712,392\n",
      "I-MONEY,6952,551\n",
      "I-ORGANIZATION,31036,4599\n",
      "I-PERCENT,44,8\n",
      "I-PERSON,16445,6104\n",
      "I-TIME,2633,248\n"
     ]
    }
   ],
   "source": [
    "#Get all the unique labels from the named entities\n",
    "labels = set([element[1] for element in named_entities])\n",
    "print(\"Entities Count of Labels and Unique Words\")\n",
    "for lbl in sorted(labels):\n",
    "    ent_list = [e[1] for e in named_entities if e[1] == lbl]\n",
    "    words = [e[0] for e in named_entities if e[1] == lbl]\n",
    "    print(str(lbl) + ',' + str(len(ent_list)) +',' + str(len(set(words))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Print Entities in tree format</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('PERSON', [('Wendy', 'NNP')]), Tree('PERSON', [('Massie', 'NNP')]), Tree('GPE', [('Pointe', 'NNP')]), Tree('PERSON', [('Baril', 'NNP')]), Tree('GPE', [('Georgian', 'NNP')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('PERSON', [('Wendy', 'NNP')]), Tree('GPE', [('Blood', 'NN')]), Tree('PERSON', [('Wendy', 'NNP')]), Tree('PERSON', [('Wendy', 'NNP')]), Tree('PERSON', [('Massie', 'NNP')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('ORGANIZATION', [('Jeongson', 'NNP'), ('Alpine', 'NNP'), ('Centre', 'NNP')]), Tree('ORGANIZATION', [('Pyeongchang', 'NNP'), ('Paralympics', 'NNPS')]), Tree('GPE', [('New', 'NNP'), ('Zealand', 'NNP')]), Tree('PERSON', [('Carl', 'NNP'), ('Murphy', 'NNP')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('GPE', [('Beijing', 'NNP')]), Tree('GPE', [('Finland', 'NNP')]), Tree('PERSON', [('Matti', 'NNP')]), Tree('GPE', [('Massie', 'NNP')]), Tree('GPE', [('American', 'NNP')]), Tree('PERSON', [('Brenna', 'NNP'), ('Huckaby', 'NNP')]), Tree('GPE', [('Sport', 'NNP')]), Tree('PERSON', [('Swimsuit', 'NNP'), ('Issue', 'NNP')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('GPE', [('Barrie', 'NNP')]), Tree('ORGANIZATION', [('Toronto', 'NNP')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('ORGANIZATION', [('ER', 'NNP')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('PERSON', [('Wendy', 'NNP')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('ORGANIZATION', [('Barrie', 'NNP'), ('North', 'NNP'), ('Collegiate', 'NNP')]), Tree('PERSON', [('Wendy', 'NNP')]), Tree('GPE', [('Massie', 'NNP')]), Tree('GPE', [('Paralympic', 'NNP')]), Tree('GPE', [('Sochi', 'NNP')]), Tree('PERSON', [('Massie', 'NNP')]), Tree('GPE', [('Pyeongchang', 'NNP')]), Tree('PERSON', [('James', 'NNP')]), Tree('GPE', [('Jeffrey', 'NNP')]), Tree('PERSON', [('Andrew', 'NNP')]), Tree('PERSON', [('Massie', 'NNP')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('GPE', [('Canadian', 'JJ')]), Tree('PERSON', [('Alex', 'NNP')]), Tree('GPE', [('Credit', 'NN')]), Tree('ORGANIZATION', [('CANADIAN', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import *\n",
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        #sentence = re.sub(r'.\\-.','_',sentence)\n",
    "        chunks = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "    return entities\n",
    "\n",
    "#Print count of entities (tree form - complete entity name)\n",
    "ent_tree = extract_entities(document[183])\n",
    "print(ent_tree)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
