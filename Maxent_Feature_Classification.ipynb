{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating features for feeding into max entropy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    " \n",
    "\n",
    "def tags_since_dt(sentence, i):\n",
    "    tags = set()\n",
    "    for word, pos in sentence[:i]:\n",
    "        if pos == 'DT':\n",
    "            tags = set()\n",
    "        else:\n",
    "            tags.add(pos)\n",
    "    return '+'.join(sorted(tags))\n",
    "    \n",
    "def features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "        prevprevword, prevprevpos = \"<START2>\", \"<START2>\"\n",
    "        history = \"<START> <START2>\"\n",
    "        i = i + 2\n",
    "        previob = history[i - 2]\n",
    "    elif i == 1:\n",
    "        prevprevword, prevprevpos = \"<START2>\", \"<START2>\"\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "        i = i + 1\n",
    "        history = \"<START>\"\n",
    "        previob = history[i - 1]\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "        prevprevword, prevprevpos = sentence[i-2]\n",
    "        previob = history[i - 1]\n",
    "    if i == len(sentence)-1:\n",
    "        nextword, nextpos = \"<END>\", \"<END>\"\n",
    "        nextnextword, nextnextpos = \"<END2>\", \"<END2>\"\n",
    "        previob = history[i - 1]\n",
    "    elif i == len(sentence)-2:\n",
    "        nextnextword, nextnextpos = \"<END2>\", \"<END2>\"\n",
    "        nextword, nextpos = sentence[i+1]\n",
    "        previob = history[i - 1]\n",
    "    else:\n",
    "        nextword, nextpos = sentence[i+1]\n",
    "        nextnextword, nextnextpos = sentence[i+2]\n",
    "        previob = history[i - 1]\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    #prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    #nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = nextword[0] in string.ascii_uppercase\n",
    "    return {\"pos\": pos,\n",
    "            \"word\": word,\n",
    "            \"lemma\": stemmer.stem(word[0]),\n",
    "            #\"isnumeric\": word[0].isdigit,\n",
    "            \"prevpos\": prevpos,\n",
    "            \"prevprevpos\": prevprevpos,\n",
    "            \"nextpos\": nextpos,\n",
    "            \"previob\": previob,\n",
    "            \"nextnextpos\": nextnextpos,\n",
    "            \"prevpos+pos\": \"%s+%s\" % (prevpos, pos),\n",
    "            \"prevprevpost+prevpos+pos\": \"%s+%s+%s\" % (prevprevpos, prevpos, pos),\n",
    "            \"pos+nextpos\": \"%s+%s\" % (pos, nextpos),\n",
    "            \"pos+nextpos+nextnextpos\": \"%s+%s+%s\" % (pos, nextpos, nextnextpos),\n",
    "            \"capitalized\": capitalized,\n",
    "            \"prevcapitalized\": prevcapitalized,\n",
    "            \"nextcapitalized\": nextcapitalized,\n",
    "            \"tags-since-dt\": tags_since_dt(sentence, i)\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the chunker classifier with custom features as mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "class NamedEntityChunkTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            #untagged_sent = [(word,tag) for word,tag,iob in sent]\n",
    "\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        nltk.config_megam('C:\\megam_0.92\\megam.exe')\n",
    "        self.classifier = nltk.MaxentClassifier.train( \n",
    "            train_set, algorithm='megam', trace=0)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class NamedEntityChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = NamedEntityChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data pre formatted and tagged (POS + IOB)  gold master data with ConllCorpusReader and creating list of sentences in Conll IOB format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "  \n",
    "from nltk.corpus import ConllCorpusReader\n",
    "my_corpus = ConllCorpusReader('.\\dataset4', '.*\\.txt', columntypes=('words', 'pos','chunk'), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sents = list(my_corpus.iob_sents())\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for sent in sents:\n",
    "    reader = [((word,tag),iob) for word,tag,iob in sent]\n",
    "    sentences.append(reader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the data in training and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47903\n",
      "20531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_pct=0.3\n",
    "training_samples, test_samples = train_test_split(sentences,test_size=test_pct)\n",
    "print(len(training_samples))\n",
    "print(len(test_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the classifier on training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = NamedEntityChunker(training_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluating the classifier on test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = chunker.evaluate([conlltags2tree([(w, t, iob) for (w, t), iob in iobs]) for iobs in test_samples])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8813096808256099\n"
     ]
    }
   ],
   "source": [
    "print(score.accuracy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
