{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition using Deep Learning\n",
    "\n",
    "Instead of using the traditional NLP approach for NER (which is similar to POS tagging) we will use a Deep Learning approach, using Tensorflow and Keras to build a simple model.\n",
    "\n",
    "We will use different embeddings (word2vec, doc2vec, GloVe), network layers and parameters in order to compare performance.\n",
    "\n",
    "Inspired in the famous blog post \"Embed, encode, attend, predict\" (https://explosion.ai/blog/deep-learning-formula-nlp), the high level of the network structure is the following:\n",
    "\n",
    "1. Hot-encoding\n",
    "2. Word embeddings\n",
    "3. LSTM layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The first step is to load a simple dataset to build a small network and try out the concept.\n",
    "\n",
    "Using **ConllCorpusReader** from NLTK simplifies data consumption and transformation, as it can parse the data format with multiple tags (POS tags, IOB tags) and create the multiple versions of the data we need for the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70554 sentences in corpus\n",
      "70554 sentences in corpus\n",
      "70554 sentences in corpus\n",
      "1685626 words in corpus\n",
      "1685626 IOB tags in corpus\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import ConllCorpusReader\n",
    "my_corpus = ConllCorpusReader('C:\\Data', '.*\\.txt', columntypes=('words', 'pos','chunk'), encoding=\"utf-8\")\n",
    "\n",
    "# Read the data in different formats (all words, by sentence, all tags, tags by sentence)\n",
    "all_data = [((word,tag),iob) for word,tag,iob in my_corpus.iob_words()]\n",
    "all_words = my_corpus.words()\n",
    "all_tags = [iob for word,tag,iob in my_corpus.iob_words()]\n",
    "all_sents = [sent for sent in my_corpus.iob_sents()]\n",
    "\n",
    "sentences = list()\n",
    "tags = list()\n",
    "for sent in all_sents:\n",
    "    word_reader = [word for word, tag, iob in sent]\n",
    "    tag_reader = [iob for word, tag, iob in sent]\n",
    "    sentences.append(' '.join(word_reader))\n",
    "    tags.append(tag_reader)\n",
    "\n",
    "# Couple of control print outs to check all dimensions match correctly\n",
    "print(len(all_sents), \"sentences in corpus\")\n",
    "print(len(sentences), \"sentences in corpus\")\n",
    "print(len(tags), \"sentences in corpus\")\n",
    "print(len(all_words), \"words in corpus\")\n",
    "print(len(all_tags), \"IOB tags in corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of pre-processing we need to encode and pad the text sentences.\n",
    "\n",
    "**Encoding**: transforming all words into integers. We need to identify all unique words, and translate each one of them to their integer representation. Same with the IOB tags, identify unique tags that are part of the source files and then map them to integers.\n",
    "\n",
    "**Padding**: To make all data items the same size, we identify the longest sentence in the set and we pad all other sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44797 vocab size in corpus\n",
      "(70554, 432)\n",
      "432 max sentence length \n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# T\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)\n",
    "word_index = t.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print(vocab_size, \"vocab size in corpus\")\n",
    "encoded_docs = t.texts_to_sequences(sentences)\n",
    "\n",
    "max_sentlen = max([len(x) for x in encoded_docs])\n",
    "padded_sentences = pad_sequences(encoded_docs, maxlen=max_sentlen, padding='post')\n",
    "print(padded_sentences.shape)\n",
    "print(max_sentlen, \"max sentence length \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get the labels (IOB tags) for the text. This requires some more additional pre-processing.\n",
    "\n",
    "**One-hot encoding**: As the neural network works better with discrete single outputs, we need to one-hot encode the labels. For each label (that we transformed first into a integer, out of all the unique possible labels) we create an array of \"bits\" that will be 1 (on) or 0 (off) depending on the type of tag:\n",
    "\n",
    "*For example:*\n",
    "\n",
    "O: outside tag   - 1 \\[1 0 0 0\\]\n",
    "\n",
    "I: inside tag    - 2 \\[0 1 0 0\\]\n",
    "\n",
    "B: beginning tag - 3 \\[0 0 1 0\\]\n",
    "\n",
    "etc.\n",
    "\n",
    "**Padding**: Padding is also required here, to the length of the longest sentence, to make both the sentences and tags matrices of matching shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1529cd8bb18f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# IOB labels padding, to the max sentence lenght\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mpadded_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sentlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shape of padded tags:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadded_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bnajlis\\Anaconda3\\envs\\dnn-ner\\lib\\site-packages\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bnajlis\\Anaconda3\\envs\\dnn-ner\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mones\u001b[1;34m(shape, dtype, order)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \"\"\"\n\u001b[1;32m--> 188\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a list of unique labels (IOB tags)\n",
    "unique_list = []\n",
    "max_label = 0\n",
    "# traverse for all elements\n",
    "for x in all_tags:\n",
    "    # check if exists in unique_list or not\n",
    "    if x not in unique_list:\n",
    "        # if does not exist in list of unique, add it\n",
    "        unique_list.append(x)\n",
    "        # count how many unique tags\n",
    "        max_label = max_label + 1\n",
    "\n",
    "# create a hash of unique IOB tags\n",
    "label_index = {label: (index + 1) for index, label in enumerate(unique_list)}\n",
    "\n",
    "# create a one-hot list of specified length with specified position turned on\n",
    "def onehot_label(length, hot_index):\n",
    "    onehot = list()\n",
    "    ind = 0\n",
    "    \n",
    "    for i in range(length):\n",
    "        # set value to 1 for hot_index\n",
    "        if ind == hot_index:\n",
    "            onehot.append(1)\n",
    "        else:\n",
    "            # everything else is 0\n",
    "            onehot.append(0)\n",
    "        ind = ind + 1\n",
    "    return onehot\n",
    "\n",
    "# Encode all IOB tags into the matrix required for the network\n",
    "ll = list()\n",
    "# iterate through all the sentences\n",
    "for s in tags:\n",
    "    l = list()\n",
    "    # iterate through all the tags\n",
    "    for t in s:\n",
    "        # add a new one-hot encoded vector for the label\n",
    "        l.append(onehot_label(max_label,label_index[t]))\n",
    "    ll.append(l)\n",
    "    \n",
    "# IOB labels padding, to the max sentence length\n",
    "padded_labels = pad_sequences(ll, maxlen=max_sentlen, padding='post')\n",
    "print(\"Shape of padded tags:\", padded_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GloVe embeddings\n",
    "\n",
    "Instead of training our own embeddings, this steps loads pre-trained embeddings.\n",
    "\n",
    "The GloVe embedding data has couple of versions, here we will use the smaller 6 billion words dataset [available here](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "Different GloVe embeddings are trained on different datasets (i.e.: Wikipedia, web crawls, Twitter). In this case we chose to use embeddings trained on Wikipedia, as we think they have a more general domain that can be better for Name Entity Recognition tagging. \n",
    "\n",
    "There are also multiple versions of the same Wikipedia embeddings, with different number of dimensions (50, 100, 200 and 300 ). Here we use the 100 dimensions file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "(519, 100)\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "# load the embedding file into memory\n",
    "embeddings_index = dict()\n",
    "f = open('C:\\\\GloVe\\\\6B\\\\glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# find the embedding vector dimensions size from the most common word in english: 'the'\n",
    "# http://www.dictionary.com/e/commonwords/\n",
    "embedding_size = len(embeddings_index['the'])\n",
    "\n",
    "sent_size = len(all_sents)\n",
    "\n",
    "# Translate our words with the embedding records:\n",
    "# create a weight matrix for words in training docs\n",
    "# based on the embeddings\n",
    "embedding_matrix = zeros((vocab_size, embedding_size))\n",
    "print(embedding_matrix.shape)\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Model Creation using Keras\n",
    "\n",
    "After we got all the data transformed into the shapes required for a model, we can create the DNN model using Keras.\n",
    "\n",
    "The model has just four layers:\n",
    "\n",
    "1. **Input: Embedding.** Here is where we include the embedding vectors as the weights.\n",
    "2. **Hidden: LSTM.**\n",
    "3. **Hidden: Dropout.**\n",
    "4. **Output: Dense.** Use sigmoid as activation function.\n",
    "\n",
    "The difference with other models here, is that we pass the embeddings matrix as the weights, so we will use the GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sentences: (61, 51)\n",
      "Shape of IOB tags: (61, 51, 9)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 51, 100)           51900     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 51, 9)             3960      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 51, 9)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 51, 9)             90        \n",
      "=================================================================\n",
      "Total params: 55,950\n",
      "Trainable params: 55,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "hidden_size = max_label\n",
    "out_size = len(label_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Here we add the GloVe embeddings as the weights parameter\n",
    "model.add(Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_sentlen, mask_zero=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))  \n",
    "model.add(Dropout(0.1))\n",
    "model.add(TimeDistributed(Dense(max_label, activation='sigmoid')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Shape of sentences:\", padded_sentences.shape)\n",
    "print(\"Shape of IOB tags:\", padded_labels.shape)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train / test sets\n",
    "\n",
    "In order to do model performance evaluation, this splits the data into a training and test sets. Split percentage can be adjusted but here is set at the traditional 70%-30%. We also fixed the random seed to get a consistent split through runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences shape: (42, 51)\n",
      "Training tags shape: (42, 51, 9)\n",
      "Test sentences shape: (18, 51)\n",
      "Test tags shape: (18, 51, 9)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "n_samples = len(all_sents)\n",
    "train_pct = 0.7\n",
    "test_pct = 1 - train_pct\n",
    "meaning_of_life = 42\n",
    "sentences_train, sentences_test, tags_train, tags_test = train_test_split(padded_sentences, padded_labels, test_size=int(test_pct*n_samples), train_size=int(train_pct*n_samples), random_state=meaning_of_life)\n",
    "\n",
    "print(\"Training sentences shape:\", sentences_train.shape)\n",
    "print(\"Training tags shape:\",tags_train.shape)\n",
    "print(\"Test sentences shape:\", sentences_test.shape)\n",
    "print(\"Test tags shape:\", tags_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation\n",
    "\n",
    "Now we run the model through 1 to 10 epochs to measure and plot accuracy improvements. Batch size is fixed at 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 4ms/step\n",
      "Epochs: 1 - Accuracy 17.741936445236206 %\n",
      "18/18 [==============================] - 0s 6ms/step\n",
      "Epochs: 2 - Accuracy 57.52689838409424 %\n",
      "18/18 [==============================] - 0s 7ms/step\n",
      "Epochs: 3 - Accuracy 89.51615691184998 %\n",
      "18/18 [==============================] - 0s 9ms/step\n",
      "Epochs: 4 - Accuracy 93.2796061038971 %\n",
      "18/18 [==============================] - 0s 10ms/step\n",
      "Epochs: 5 - Accuracy 93.54841709136963 %\n",
      "18/18 [==============================] - 0s 11ms/step\n",
      "Epochs: 6 - Accuracy 93.54841709136963 %\n",
      "18/18 [==============================] - 0s 14ms/step\n",
      "Epochs: 7 - Accuracy 93.54841709136963 %\n",
      "18/18 [==============================] - 0s 15ms/step\n",
      "Epochs: 8 - Accuracy 93.54841709136963 %\n",
      "18/18 [==============================] - 0s 23ms/step\n",
      "Epochs: 9 - Accuracy 93.54841709136963 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          0.17741936445236206,
          0.5752689838409424,
          0.8951615691184998,
          0.932796061038971,
          0.9354841709136963,
          0.9354841709136963,
          0.9354841709136963,
          0.9354841709136963,
          0.9354841709136963
         ]
        }
       ],
       "layout": {
        "title": "Accuracy vs Training Epochs"
       }
      },
      "text/html": [
       "<div id=\"ac4bc705-1cc9-4baa-8022-9d8eacad3cce\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ac4bc705-1cc9-4baa-8022-9d8eacad3cce\", [{\"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [0.17741936445236206, 0.5752689838409424, 0.8951615691184998, 0.932796061038971, 0.9354841709136963, 0.9354841709136963, 0.9354841709136963, 0.9354841709136963, 0.9354841709136963]}], {\"title\": \"Accuracy vs Training Epochs\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"ac4bc705-1cc9-4baa-8022-9d8eacad3cce\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ac4bc705-1cc9-4baa-8022-9d8eacad3cce\", [{\"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"y\": [0.17741936445236206, 0.5752689838409424, 0.8951615691184998, 0.932796061038971, 0.9354841709136963, 0.9354841709136963, 0.9354841709136963, 0.9354841709136963, 0.9354841709136963]}], {\"title\": \"Accuracy vs Training Epochs\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Model performance evaluation for multiple epochs\n",
    "import pandas as pd\n",
    "import plotly\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "batch_size = 20\n",
    "epochs = list()\n",
    "accuracies = list()\n",
    "\n",
    "for n_epochs in range(1, 10):\n",
    "    epochs.append(n_epochs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(sentences_train, tags_train, batch_size=batch_size, epochs=n_epochs, verbose=0)\n",
    "    loss, accuracy = model.evaluate(sentences_test, tags_test)\n",
    "    accuracies.append(accuracy)\n",
    "    print('Epochs:', n_epochs, '- Accuracy', accuracy * 100, \"%\")\n",
    "    \n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "plotly.offline.iplot({\n",
    "    \"data\": [Scatter(x=epochs, y=accuracies)],\n",
    "    \"layout\": Layout(title=\"Accuracy vs Training Epochs\")\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn-ner",
   "language": "python",
   "name": "dnn-ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
