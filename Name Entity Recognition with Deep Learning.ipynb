{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition using Deep Learning\n",
    "\n",
    "Instead of using the traditional NLP approach for NER (which is similar to POS tagging) we will use a Deep Learning approach, using Tensorflow and Keras to build a simple model.\n",
    "\n",
    "We will use different embeddings (word2vec, doc2vec, GloVe), network layers and parameters in order to compare performance.\n",
    "\n",
    "Inspired in the famous blog post \"Embed, encode, attend, predict\" (https://explosion.ai/blog/deep-learning-formula-nlp), the high level of the network structure is the following:\n",
    "\n",
    "1. Hot-encoding\n",
    "2. Word embeddings\n",
    "3. LSTM layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The first step is to load a simple dataset to build a small network and try out the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 sentences in corpus\n",
      "61 sentences in corpus\n",
      "61 sentences in corpus\n",
      "1486 words in corpus\n",
      "1486 IOB tags in corpus\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import ConllCorpusReader\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "my_corpus = ConllCorpusReader('C:\\Data', '.*\\.txt', columntypes=('words', 'pos','chunk'), encoding=\"utf-8\")\n",
    "\n",
    "subset = '2.txt'\n",
    "# Skip reading the POS tags, just read the word and IOB NER tags\n",
    "all_data = [((word,tag),iob) for word,tag,iob in my_corpus.iob_words(subset)]\n",
    "all_words = my_corpus.words(subset)\n",
    "all_tags = [iob for word,tag,iob in my_corpus.iob_words(subset)]\n",
    "all_sents = [sent for sent in my_corpus.iob_sents(subset)]\n",
    "\n",
    "sentences = list()\n",
    "tags = list()\n",
    "for sent in all_sents:\n",
    "    word_reader = [word for word, tag, iob in sent]\n",
    "    tag_reader = [iob for word, tag, iob in sent]\n",
    "    sentences.append(' '.join(word_reader))\n",
    "    tags.append(tag_reader)\n",
    "\n",
    "print(len(all_sents), \"sentences in corpus\")\n",
    "print(len(sentences), \"sentences in corpus\")\n",
    "print(len(tags), \"sentences in corpus\")\n",
    "print(len(all_words), \"words in corpus\")\n",
    "print(len(all_tags), \"IOB tags in corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to encode and pad the text sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519 vocab size in corpus\n",
      "(61, 51)\n",
      "51 max sentence length \n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)\n",
    "word_index = t.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print(vocab_size, \"vocab size in corpus\")\n",
    "encoded_docs = t.texts_to_sequences(sentences)\n",
    "\n",
    "max_sentlen = max([len(x) for x in encoded_docs])\n",
    "padded_sentences = pad_sequences(encoded_docs, maxlen=max_sentlen, padding='post')\n",
    "print(padded_sentences.shape)\n",
    "print(max_sentlen, \"max sentence length \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get the labels for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 51, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a list of unique labels\n",
    "unique_list = []\n",
    "max_label = 0\n",
    "# traverse for all elements\n",
    "for x in all_tags:\n",
    "    # check if exists in unique_list or not\n",
    "    if x not in unique_list:\n",
    "        unique_list.append(x)\n",
    "        max_label = max_label + 1\n",
    "\n",
    "label_index = {label: (index + 1) for index, label in enumerate(unique_list)}\n",
    "\n",
    "def onehot_label(length, hot_index):\n",
    "    onehot = list()\n",
    "    ind = 0\n",
    "    for i in range(length):\n",
    "        if ind == hot_index:\n",
    "            onehot.append(1)\n",
    "        else:\n",
    "            onehot.append(0)\n",
    "        ind = ind + 1\n",
    "    return onehot\n",
    "\n",
    "#print(onehot_label(15,2))\n",
    "\n",
    "# encode labels\n",
    "ll = list()\n",
    "for s in tags:\n",
    "    l = list()\n",
    "    for t in s:\n",
    "        #l.append(label_index[t])\n",
    "        l.append(onehot_label(max_label,label_index[t]))\n",
    "    ll.append(l)\n",
    "    \n",
    "#pad labels\n",
    "padded_labels = pad_sequences(ll, maxlen=max_sentlen, padding='post')\n",
    "print(padded_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GloVe embeddings\n",
    "\n",
    "The GloVe embedding data has couple of versions, first we will use the smaller 6 billion words dataset [available here](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('C:\\data\\GloVe\\\\6B\\glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "embedding_size = len(embeddings_index['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(519, 100)\n",
      "10\n",
      "[-0.071953    0.23127     0.023731   -0.50638002  0.33923     0.19589999\n",
      " -0.32943001  0.18364    -0.18057001  0.28963     0.20448001 -0.54960001\n",
      "  0.27399001  0.58327001  0.20468    -0.49228001  0.19973999 -0.070237\n",
      " -0.88049001  0.29484999  0.14071    -0.1009      0.99449003  0.36973\n",
      "  0.44554001  0.28997999 -0.1376     -0.56365001 -0.029365   -0.4122\n",
      " -0.25268999  0.63181001 -0.44767001  0.24363001 -0.10813     0.25163999\n",
      "  0.46967     0.37549999 -0.23613    -0.14128999 -0.44536999 -0.65736997\n",
      " -0.042421   -0.28636    -0.28810999  0.063766    0.20281    -0.53542\n",
      "  0.41306999 -0.59722    -0.38613999  0.19389001 -0.17809001  1.66180003\n",
      " -0.011819   -2.3736999   0.058427   -0.26980001  1.2823      0.81924999\n",
      " -0.22322001  0.72931999 -0.053211    0.43507001  0.85010999 -0.42934999\n",
      "  0.92663997  0.39050999  1.05850005 -0.24561    -0.18265    -0.53280002\n",
      "  0.059518   -0.66018999  0.18990999  0.28836    -0.24339999  0.52784002\n",
      " -0.65762001 -0.14081     1.04910004  0.51340002 -0.23816     0.69894999\n",
      " -1.4813     -0.24869999 -0.17936    -0.059137   -0.08056    -0.48782\n",
      "  0.014487   -0.62589997 -0.32367     0.41861999 -1.08070004  0.46742001\n",
      " -0.49930999 -0.71894997  0.86894     0.19539   ]\n"
     ]
    }
   ],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "sent_size = len(all_sents)\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "print(embedding_matrix.shape)\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(word_index['that'])\n",
    "print(embedding_matrix[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model Creation\n",
    "\n",
    "After we manipulated all the data and transform it in the shape required for a model, we create the DNN model.\n",
    "\n",
    "The difference with other models here, is that we pass the embeddings matrix as the weights, so we will use the GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sentences: (61, 51)\n",
      "Shape of IOB tags: (61, 51, 9)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 51, 100)           51900     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 51, 9)             3960      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 51, 9)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 51, 9)             90        \n",
      "=================================================================\n",
      "Total params: 55,950\n",
      "Trainable params: 55,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "hidden_size = max_label\n",
    "out_size = len(label_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "# Here we add the GloVe embeddings as the weights parameter\n",
    "model.add(Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_sentlen, mask_zero=True))\n",
    "\n",
    "model.add(LSTM(hidden_size, return_sequences=True))  \n",
    "model.add(Dropout(0.1))\n",
    "model.add(TimeDistributed(Dense(max_label, activation='sigmoid')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Shape of sentences:\", padded_sentences.shape)\n",
    "print(\"Shape of IOB tags:\", padded_labels.shape)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train / test sets\n",
    "\n",
    "In order to do model performance evaluation, this split the model into a training and test sets. Percentage can be adjusted (set at the traditional 70%-30%) and also fixed the random seed so set split is always consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences shape: (42, 51)\n",
      "Training tags shape: (42, 51, 9)\n",
      "Test sentences shape: (18, 51)\n",
      "Test tags shape: (18, 51, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bnajlis\\Anaconda3\\envs\\dnn-ner\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "n_samples = len(all_sents)\n",
    "train_pct = 0.7\n",
    "test_pct = 1 - train_pct\n",
    "meaning_of_life = 42\n",
    "sentences_train, sentences_test, tags_train, tags_test = train_test_split(padded_sentences, padded_labels, test_size=int(test_pct*n_samples), train_size=int(train_pct*n_samples), random_state=meaning_of_life)\n",
    "\n",
    "print(\"Training sentences shape:\", sentences_train.shape)\n",
    "print(\"Training tags shape:\",tags_train.shape)\n",
    "print(\"Test sentences shape:\", sentences_test.shape)\n",
    "print(\"Test tags shape:\", tags_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2855d688d30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "n_epochs = 100\n",
    "model.fit(sentences_train, tags_train, batch_size=batch_size, epochs=n_epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 3ms/step\n",
      "Loss: 0.505577\n",
      "Accuracy 93.5511827468872 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "loss, accuracy = model.evaluate(sentences_train, tags_train)\n",
    "print('Loss: %f' % loss)\n",
    "print('Accuracy', accuracy * 100, \"%\")\n",
    "\n",
    "predictions = model.predict_classes(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn-ner",
   "language": "python",
   "name": "dnn-ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
