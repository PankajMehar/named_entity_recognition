{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition using Deep Learning\n",
    "\n",
    "Instead of using the traditional NLP approach for NER (which is similar to POS tagging) we will use a Deep Learning approach, using Tensorflow and Keras to build a simple model.\n",
    "\n",
    "We will use different embeddings (word2vec, doc2vec, GloVe), network layers and parameters in order to compare performance.\n",
    "\n",
    "Inspired in the famous blog post \"Embed, encode, attend, predict\" (https://explosion.ai/blog/deep-learning-formula-nlp), the high level of the network structure is the following:\n",
    "\n",
    "1. Hot-encoding\n",
    "2. Word embeddings\n",
    "3. LSTM layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The first step is to load a simple dataset to build a small network and try out the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 sentences in corpus\n",
      "58 sentences in corpus\n",
      "58 sentences in corpus\n",
      "1491 words in corpus\n",
      "1491 IOB tags in corpus\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import ConllCorpusReader\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "my_corpus = ConllCorpusReader('C:\\Data', '.*\\.txt', columntypes=('words', 'pos','chunk'), encoding=\"utf-8\")\n",
    "my_corpus.iob_words('2.txt')\n",
    "\n",
    "# Skip reading the POS tags, just read the word and IOB NER tags\n",
    "all_data = [((word,tag),iob) for word,tag,iob in my_corpus.iob_words('2.txt')]\n",
    "all_words = my_corpus.words('2.txt')\n",
    "all_tags = [iob for word,tag,iob in my_corpus.iob_words('2.txt')]\n",
    "all_sents = [sent for sent in my_corpus.iob_sents('2.txt')]\n",
    "\n",
    "sentences = list()\n",
    "tags = list()\n",
    "for sent in all_sents:\n",
    "    word_reader = [word for word, tag, iob in sent]\n",
    "    tag_reader = [iob for word, tag, iob in sent]\n",
    "    sentences.append(' '.join(word_reader))\n",
    "    tags.append(tag_reader)\n",
    "\n",
    "print(len(all_sents), \"sentences in corpus\")\n",
    "print(len(sentences), \"sentences in corpus\")\n",
    "print(len(tags), \"sentences in corpus\")\n",
    "print(len(all_words), \"words in corpus\")\n",
    "print(len(all_tags), \"IOB tags in corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to encode and pad the text sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520 vocab size in corpus\n",
      "(58, 72)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)\n",
    "word_index = t.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print(vocab_size, \"vocab size in corpus\")\n",
    "encoded_docs = t.texts_to_sequences(sentences)\n",
    "\n",
    "max_sentlen = max([len(x) for x in encoded_docs])\n",
    "padded_sentences = pad_sequences(encoded_docs, maxlen=max_sentlen, padding='post')\n",
    "print(padded_sentences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get the labels for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 72, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a list of unique labels\n",
    "unique_list = []\n",
    "max_label = 0\n",
    "# traverse for all elements\n",
    "for x in all_tags:\n",
    "    # check if exists in unique_list or not\n",
    "    if x not in unique_list:\n",
    "        unique_list.append(x)\n",
    "        max_label = max_label + 1\n",
    "\n",
    "label_index = {label: (index + 1) for index, label in enumerate(unique_list)}\n",
    "\n",
    "def onehot_label(length, hot_index):\n",
    "    onehot = list()\n",
    "    ind = 0\n",
    "    for i in range(length):\n",
    "        if ind == hot_index:\n",
    "            onehot.append(1)\n",
    "        else:\n",
    "            onehot.append(0)\n",
    "        ind = ind + 1\n",
    "    return onehot\n",
    "\n",
    "#print(onehot_label(15,2))\n",
    "\n",
    "# encode labels\n",
    "ll = list()\n",
    "for s in tags:\n",
    "    l = list()\n",
    "    for t in s:\n",
    "        #l.append(label_index[t])\n",
    "        l.append(onehot_label(max_label,label_index[t]))\n",
    "    ll.append(l)\n",
    "    \n",
    "#pad labels\n",
    "padded_labels = pad_sequences(ll, maxlen=max_sentlen, padding='post')\n",
    "print(padded_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GloVe embeddings\n",
    "\n",
    "The GloVe embedding data has couple of versions, first we will use the smaller 6 billion words dataset [available here](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('C:\\data\\GloVe\\\\6B\\glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "embedding_size = len(embeddings_index['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(520, 100)\n",
      "10\n",
      "[-0.071953    0.23127     0.023731   -0.50638002  0.33923     0.19589999\n",
      " -0.32943001  0.18364    -0.18057001  0.28963     0.20448001 -0.54960001\n",
      "  0.27399001  0.58327001  0.20468    -0.49228001  0.19973999 -0.070237\n",
      " -0.88049001  0.29484999  0.14071    -0.1009      0.99449003  0.36973\n",
      "  0.44554001  0.28997999 -0.1376     -0.56365001 -0.029365   -0.4122\n",
      " -0.25268999  0.63181001 -0.44767001  0.24363001 -0.10813     0.25163999\n",
      "  0.46967     0.37549999 -0.23613    -0.14128999 -0.44536999 -0.65736997\n",
      " -0.042421   -0.28636    -0.28810999  0.063766    0.20281    -0.53542\n",
      "  0.41306999 -0.59722    -0.38613999  0.19389001 -0.17809001  1.66180003\n",
      " -0.011819   -2.3736999   0.058427   -0.26980001  1.2823      0.81924999\n",
      " -0.22322001  0.72931999 -0.053211    0.43507001  0.85010999 -0.42934999\n",
      "  0.92663997  0.39050999  1.05850005 -0.24561    -0.18265    -0.53280002\n",
      "  0.059518   -0.66018999  0.18990999  0.28836    -0.24339999  0.52784002\n",
      " -0.65762001 -0.14081     1.04910004  0.51340002 -0.23816     0.69894999\n",
      " -1.4813     -0.24869999 -0.17936    -0.059137   -0.08056    -0.48782\n",
      "  0.014487   -0.62589997 -0.32367     0.41861999 -1.08070004  0.46742001\n",
      " -0.49930999 -0.71894997  0.86894     0.19539   ]\n"
     ]
    }
   ],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "sent_size = len(all_sents)\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "print(embedding_matrix.shape)\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(word_index['that'])\n",
    "print(embedding_matrix[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 72)\n",
      "(58, 72, 11)\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 2.4638\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 2.4014\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.3433\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 2.2893\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.2356\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 2.1785\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 2.1189\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 2.0576\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 1.9909\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 1.9248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2926a18aac8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "import numpy as np\n",
    "\n",
    "hidden_size = 11\n",
    "out_size = len(label_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=max_sentlen, mask_zero=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))  \n",
    "#model.add(TimeDistributedDense(out_size))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "print(padded_sentences.shape)\n",
    "print(padded_labels.shape)\n",
    "\n",
    "batch_size = 32\n",
    "model.fit(padded_sentences, padded_labels, batch_size=batch_size, epochs=10)#, validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn-ner",
   "language": "python",
   "name": "dnn-ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
